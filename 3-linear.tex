\chapter{Linear Endomorphisms}

\section{Modules Over Polynomial Domains}

In this section we will assume the conditions and notations of Examples~\ref{xmpl:vector-space-as-k[x]-module-1}, \ref{xmpl:vector-space-as-k[x]-module-2} and \ref{xmpl:vector-space-as-k[x]-module-3}.

\begin{thm}\label{thm:similar-operators}
    Let\/ $\Delta$ and\/ $\Lambda$ be endomorphisms of\/ $\V$. Then, a\/ $\kappa$-linear endomorphism\/ $\phi\colon\V\to\V$ is a module isomorphism from\/ $\V_\Delta$ to\/ $\V_\Lambda$ if, and only if, it is a\/ $\kappa$-automorphism of\/ $\V$ that satisfies
    $$
        \Lambda = \phi\Delta\phi^{-1}.
    $$
\end{thm}

\begin{proof}
    By definition, $\phi\colon\V_\Delta\to\V_\Lambda$ is a morphism if, and  only if, for all $i$ and all $v\in\V$ we have $\phi(x^i\cdot v)=x^i\cdot\phi(v)$. Therefore,
    $$
        \phi\text{ is isomorphism }\iff (\forall\,i,v)\ \phi(\Delta^i(v))=\Lambda^i(\phi(v))
        \iff \phi\Delta=\Lambda\phi.
    $$
\end{proof}

\begin{ntn}
    When the conditions of the theorem are met we will write\/ $\Delta\sim\Lambda$ and say that\/ $\Delta$ and\/ $\Lambda$ are \textsl{similar}. The same notation applies to square matrices.
\end{ntn}

\begin{cor}
    Let\/ $\Delta$ and\/ $\Lambda$ be two endomorphisms of\/ $\V$. Then
    $$
        \Delta\sim\Lambda
            \iff \op{ElemDiv}(\Delta)=\op{ElemDiv}(\Lambda)
            \iff \op{InvFact}(\Delta)=\op{InvFact}(\Lambda).
    $$
\end{cor}

\begin{defn}
    The \textsl{characteristic polynomial} of $\Delta$ is defined as
    $$
        \chi_\Delta=\prod_{\substack{1\le i\le r\\1\le j\le k_i}}p_i^{e_{ij}}.
    $$
\end{defn}

\needspace{2\baselineskip}
\begin{rem}\label{rem:minimal-characteristic}
    In particular, we have
    \begin{enumerate}[-]
        \item $\dim(\V)=\deg\chi_\Delta$,
        
        \item $m_\Delta\mid\chi_\Delta$ and
        
        \item both polynomials have the same prime factors.
    \end{enumerate}
\end{rem}

\begin{defn}
    The linear operator $\Delta$ is \textsl{nonderogatory} if its minimal and characteristic polynomials are equal, i.e.,
    $$
        m_\Delta = \chi_\Delta.
    $$
    Equivalently, if
    $$
        \deg m_\Delta = \deg \chi_\Delta,
    $$
    or if
    $$
        \deg m_\Delta = \dim\V.
    $$
    Similar statements hold the matrix of $\Delta$ in any given basis.
\end{defn}

\begin{thm}\label{thm:Delta-cyclic-equivalences}
    Suppose that\/ $\Delta$ has minimal polynomial  
    $$
        m_\Delta = p_1^{e_1}\cdots p_k^{e_k},
    $$  
    where the\/ $p_i$ are distinct monic prime polynomials. Then the following are equivalent:  
    \begin{enumerate}[\rm a)]
        \item $\V_\Delta$ is cyclic.  
        
        \item $\V_\Delta$ is the direct sum  
        $$
            \V_\Delta = \lsp{v_i}\oplus\cdots\oplus\lsp{v_k},
        $$  
        of\/ $\Delta$-cyclic submodules of order\/ $p_i^{e_i}$.  
        
        \item The elementary divisors of\/ $M$ are  
        $$
            \op{ElemDiv}(M) = \set{p_1^{e_1},
                \dots,p_k^{e_k}}.
        $$  
        \item $\Delta$ is nonderogatory.  
    \end{enumerate}
\end{thm}

\begin{proof}
    This is a direct consequence of Theorem~\ref{thm:cyclic-torsion}.
\end{proof}

\begin{cor}
    Suppose that the minimal polynomial\/ $m_\Delta$ is prime and let\/ $\V'$ be a submodule of order\/ $m_\Delta$. Let\/ $\Delta'\subseteq\Delta$ the endomorphism of\/ $\V'$ induced by\/~$\Delta$. Then the following are equivalent:
    \begin{enumerate}[\rm a)]
        \item $\V'$ is cyclic.  
        \item $\V'$ is indecomposable.  
        \item $\chi_{\Delta'}$ is irreducible.  
        \item $\Delta'$ is nonderogatory.
        \item $\dim\V' = \deg m_{\Delta'}$.
    \end{enumerate}
\end{cor}

\begin{proof}First note that, according to Theorem~\ref{thm:prime-factors-of-ord-have-submodules}, such a submodule $\V'$ always exists.
    \begin{enumerate}[\rm a)]
        \item $\Rightarrow$~b) Cyclic and prime order is clearly indecomposable [cf.~Theorem~\ref{thm:indecomposable-equivalences}]

        \item $\Rightarrow$~c) By the theorem $\op{ElemDiv(\V')}=\set m_\Delta$.

        \item $\Rightarrow$~d) Trivial.

        \item $\Rightarrow$~e) Trivial because $m_{\Delta'}=m_\Delta$.

        \item $\Rightarrow$~a) Trivial.
    \end{enumerate}
\end{proof}

\begin{defn}\label{defn:companion-matrix}
    Given a monic polynomial $f(x)=x^n+a_{n-1}x^{n-1}+\cdots+a_0$ its \textsl{companion matrix} is
    $$
        C[f] = 
        \begin{bmatrix}
        0 & 0 & \cdots & 0 & -a_0 \\
        1 & 0 & \cdots & 0 & -a_1 \\
        0 & 1 & \cdots & 0 & -a_2 \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & \cdots & 1 & -a_{n-1}
        \end{bmatrix}.
    $$
    If $\V_\Delta$ is cyclic generated by $v$ and $f=\ord(v)$, then $C[f]$ is the matrix of $\Delta$ in the basis $\basis B=(v,\Delta v,\dots,\Delta^{n-1}v)$.
\end{defn}

\begin{thm}
    Let\/ $f\in\kappa[x]$ be a monic polynomial. Then
    \begin{enumerate}[\rm a)]
        \item The companion matrix $C[f]$ is nonderogatory; in fact,
        $$
            \chi_{C[f]} = m_{C[f]} = f.
        $$
        
        \item $\V_\Delta$ is cyclic if, and only if, $\Delta$ can be represented by a companion matrix, in which case the representing basis is $\Delta$-cyclic.
    \end{enumerate}
\end{thm}

\begin{proof}${}$
    \begin{enumerate}[\rm a)]
        \item Write $f=x^n+a_{n-1}x^{n-1}+\cdots+a_0$ and let $(e_1,\dots,e_n)$ be the canonic basis of $\kappa^n$. Put $C=C[f]$. Then, for $1\le i<n$, we have
        $$
            Ce_i=e_{i+1}
            \quad\text{and so}\quad
            e_{i+1} = C^ie_1.
        $$
        Thus, given $g\in\kappa[x]$, we have
        $$
            g(C)=0\implies g(C)e_1=0
        $$
        and
        $$
            g(C)e_i=0
                \implies g(C)e_{i+1}=g(C)Ce_i=Cg(C)e_i=0,
        $$
        i.e.,
        $$
            g(C)=0\iff g(C)e_1=0.
        $$
        Since
        \begin{align*}
            f(C)e_1 &= \sum_{i=0}^{n-1}a_iC^ie_1
                    +CC^{n-1}e_1\\
                &= \sum_{i=0}^{n-1}a_ie_{i+1}-Ce_n\\
                &= 0,
        \end{align*}
        we deduce that $f(C)=0$. In addition, if $\deg g<n$ then we must have $g(C)e_1\ne0$, because otherwise, that would annihilate a nontrivial linear combination of $\set{e_1,\dots,e_n}$. In conclusion, $f$ is the polynomial of smallest degree for which $f(C)=0$, i.e., $f=m_C$. In particular,
        $$
            \deg m_C=n=\deg\chi_C.
        $$

        \item As observed in Definition~\ref{defn:companion-matrix}, if $\V$ is cyclic generated by $v$, then $[\Delta]_{\basis B}$ is a companion matrix for $\basis B=(v,\Delta v,\dots,\Delta^{n-1}v)$. Conversely, if $\Delta$ is represented by a companion matrix, by part~a) such a matrix is nonderogatory and therefore $\V$ is cyclic by Theorem~\ref{thm:Delta-cyclic-equivalences}.
    \end{enumerate}
\end{proof}

\begin{defn}
     A matrix $D$ is in the \textsl{elementary divisor form} or \textsl{rational canonical form} if there are monic prime polynomials $p_1,\dots,p_r$ and positive exponents $e_1,\dots,e_r$ such that 
     $$
        D = \op{diag}(C[p_1^{e_1}],\dots,C[p_r^{e_r}]).
     $$
\end{defn}

\begin{xmpl}
   Let\/ $\Delta$ be a linear operator on the vector space\/ $\R^7$ and suppose that\/ $\Delta$ has minimal polynomial
    $$
        m_\Delta(x) = (x - 1)(x^2 + 1)^2.
    $$
    Since the sum of the degrees of all elementary divisors must equal\/ $7$, we have two possibilities:
    \begin{enumerate}[\rm i)]
        \item $x-1, \, (x^2+1)^2,\, x^2+1$ and
        \item $x-1, \, x-1, \, x-1,\,(x^2+1)^2$.
    \end{enumerate}
    For instance, the first possibility produces the matrix
    $$
        \left[
            \begin{array}{r|rrrr|rr}
                1 & 0 & 0 & 0 & 0 & 0 & 0 \\
                \noalign{\color{gray}\hrule height 0.1mm}
                0 & 0 & 0 & 0 & -1 & 0 & 0 \\
                0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                0 & 0 & 1 & 0 & -2 & 0 & 0 \\
                0 & 1 & 0 & 0 & 0 & 0 & 0 \\
                \noalign{\color{gray}\hrule height 0.1mm}
                0 & 0 & 0 & 0 & 0 & 0 & -1 \\
                0 & 0 & 0 & 0 & 0 & 1 & 0
            \end{array}
        \right]
    $$
\end{xmpl}

\begin{thm}\label{thm:companion-of-product}
    If $f$ and $g$ are coprime polynomials in $\kappa[x]$ then
    $$
        C[fg]\sim\op{diag}(C[f],C[g]).
    $$
\end{thm}

\begin{proof}
    Both matrices have the same dimension and the same minimal polynomial $fg$. Thus, both matrices are nonderogatory and therefore share the same elementary divisors, which are the power prime factors of $fg$ [cf.~Theorem~\ref{thm:Delta-cyclic-equivalences}].
\end{proof}

\begin{defn}
    A matrix $A$ is in the \textsl{invariant factor form} if there are polynomials $q_1,\dots,q_r$ such that $q_{i+1}\mid q_i$ for $1\le i<r$ and
    $$
        A = \op{diag}(C[q_1],\dots,C[q_r]).
    $$
\end{defn}

\begin{thm}
    $\chi_A(x)=\det(xI-A)$.
\end{thm}

\begin{proof}
    First suppose that $A=C[f]$ where $f(x)=x^n+a_{n-1}x^{n-1}+\cdots+a_0$. Put $g(x)=x^{n-1}+a_{n-1}x^{n-1}+\cdots+a_1$. Then
    \begin{align*}
        \det(xI-A) & = \det
            \begin{bmatrix}
                x & 0 & \cdots & 0 & a_0 \\
                -1 & x & \cdots & 0 & a_1 \\
                \vdots&\vdots&\ddots&\vdots&\vdots\\
                0 & 0 & \cdots & x & a_{n-2} \\
                0 & 0 & \cdots & -1 & x + a_{n-1}
            \end{bmatrix}\\
            &=x\chi_{C[g]}+(-1)^{n+1}a_0(-1)^{n-1}\\
            &= xg(x)+a_0\\
            &= f(x),
    \end{align*}
    where the second equality comes from induction on~$n$. Since $f=\chi_{C[f]}$, the first case is proved.

    Second, suppose that $A$ is in elementary divisor form
    $$
        A = \op{diag}(C[p_1^{e_{11}}],\dots,C[p_r^{e_{rk_r}}]).
    $$
    Then,
    $$
        \det(xI-A) = \prod_{i,j}^r\det(xI-C[p_i(x)^{e_{ij}}])
            = \prod_{i,j}^rp_i(x)^{e_{ij}} = \chi_A(x)
    $$

    Finally, for the general case take $P$ invertible such that $PAP^{-1}$ is in elementary divisor form and observe that
    $$
        \det(xI-A) = \det(xI-PAP^{-1})=\chi_{PAP^{-1}}(x)=\chi_A(x).
    $$
\end{proof}

\begin{thm}\label{thm:subfield-similarity}
    Let\/ $K\subseteq\kappa$ be the smallest subfield of\/ $\kappa$ that contains the entries of the matrix\/~$A\in M_n(\kappa)$.
    \begin{enumerate}[\rm a)]
      \item The invariant factors of\/ $A$ are polynomials over\/ $K$.
      \item A matrix with entries in\/ $K$ is similar over\/ $K$ to\/ $A$ if, and only if, it is similar to\/ $A$ over\/ $\kappa$.
    \end{enumerate}
\end{thm}

\begin{proof}
    Since the invariant factors only need to satisfy two conditions: 1)~the divisibility relation $d_i \mid d_j$ when $i<j$ and 2)~the sum of their degrees must equal $n$, the invariant factors in $K$ are also invariant factors in $\kappa$. And since the invariant factors determine the similarity class, $A$ will be similar to $B$ over $\kappa$ if, and only if, it is similar over~$K$.
\end{proof}

\begin{cor}
    The invariant factors of a matrix $A$ are rational functions in the entries of $A$.
\end{cor}

\begin{proof}
    The field of rational functions in the entries of $A$ is the smallest field that includes all these entries.
\end{proof}

\begin{xmpl}
    Over the field $\R$ the matrix
    $$
        A = \begin{pmatrix}
            0 & -1 \\
            1 & \phantom-0
            \end{pmatrix}
    $$
    is the companion matrix for the polynomial $x^2+1$, and so
    $$
        \op{ElemDiv}_\R(A) = \{x^2 + 1\} = 
            \op{InvFact}_\R(A)
    $$
    However, as a complex matrix, the rational canonical form is
    $$
        A = \begin{pmatrix}
                i & \phantom-0 \\
                0 & -i
            \end{pmatrix}.
    $$
    Therefore,
    \begin{align*}
        \op{ElemDiv}_\C(A) &= \set{x-i, x+i}\\ \op{InvFact}_\C(A) &= \set{x^2 + 1}.
    \end{align*}
\end{xmpl}

\section{Eigenvalues and Eigenvectors}

Recall that an \textsl{eigenvalue} of an endomorphism $\Delta\in\End_\kappa(\V)$ is an element $\lambda\in\kappa$ such that $\Delta - \lambda\id$ is not a monomorphism, i.e., $\lambda$ is a root of the characteristic polynomial $\chi_\Delta$ (and consequently of the minimal polynomial $m_\Delta$ too). The kernel $\ker(\Delta-\lambda\id)$ is the \textsl{eigenspace} associated to the eigenvalue~$\lambda$.

\begin{defn}
    The \textsl{algebraic multiplicity} of an eigenvalue $\lambda$ is its multiplicity as a root of $\chi_\Delta$. The \textsl{geometric multiplicity} is the dimension of its eigenspace.
\end{defn}

\begin{thm}
    The geometric multiplicity of every eigenvalue is less than or equal to its algebraic multiplicity.
\end{thm}

\begin{proof}
    Let $\lambda$ an eigenvalue of $\Delta$ and let $m_g$ denote the geometric multiplicity of $\lambda$. Let $\basis B=(v_1,\dots,v_{m_g},v_{m_g+1},\dots,v_n)$ be a basis of $\V$ extending a basis $(v_1,\dots,v_{m_g})$ the eigenspace associated to $\lambda$. Then
    $$
        [\Delta]_{\basis B}= \begin{bmatrix}
                \lambda I_{m_g} &A\\
                0 &B
            \end{bmatrix}.
    $$
    Therefore,
    $$
        \chi_\Delta(x) = (x-\lambda)^{m_g}\chi_B(x).
    $$
\end{proof}

\begin{rem}
    If\/ $v \in \V$ has order\/ $p$ and\/ $e = \deg p$, we have established that a basis of\/ $\lsp v$ is\/ $(v, x \cdot v, \dots, x^{e-1} \cdot v)$. However, if\/ $p_1, \dots, p_{e-1}$ are any polynomials satisfying\/ $\deg p_k = k$, the sequence\/ $(v, p_1 \cdot v, \dots, p_{e-1} \cdot v)$ also forms a basis of\/~$\lsp v$. In the special case where\/ $p = (x - \lambda)^e$, a valid choice for the basis is:
    $$
        \basis B=(v,(x-\lambda)\cdot v,\dots,(x-\lambda)^{e-1}\cdot v).
    $$
    Given $0\le k<e$, we have
    \begin{align*}
        \Delta((x-\lambda)^k\cdot v)
            &= x\cdot(x -\lambda)^k\cdot v\\
            &= ((x-\lambda)(x-\lambda)^k+\lambda(x-\lambda)^k)\cdot v\\
            &= (x-\lambda)^{k+1}\cdot v +\lambda(x-\lambda)^k\cdot v,
    \end{align*}
    which implies that
    \begin{equation}\label{eq:jordan-block}
        [\Delta_{\lsp v}]_{\basis B} = \begin{bmatrix}
                \lambda & 0 & \cdots & \cdots & 0 \\
                1 & \lambda & \ddots &  & \vdots \\
                0 & 1 & \ddots & \ddots & \vdots \\
                \vdots & \ddots & \ddots & \ddots & 0 \\
                0 & \cdots & 0 & 1 & \lambda
            \end{bmatrix}
    \end{equation}
    where the $e\times e$ matrix is called a \textsl{Jordan block} associated with $\lambda$ and denoted by $\mathcal J(\lambda,e)$.
    
    Since $C[(x-\lambda)^e]$ and $\mathcal J(\lambda,e)$ represent the same endomorphism $\Delta_{\lsp v}$ in different bases, we deduce that $C[(x-\lambda)^e]\sim\mathcal J(\lambda,e)$. In consequence,
\end{rem}

\begin{thm} \label{thm:jordan-form}{\rm[Jordan Canonical Form]}
    Suppose that the minimal polynomial of\/ $\Delta \in \End_\kappa(V)$ splits over the base field\/ $\kappa$, that is,
    $$
        m_\Delta(x) = (x-\lambda_1)^{e_1}
            \cdots(x-\lambda_r)^{e_r}.
    $$
    Then,
    \begin{enumerate}[\rm a)]
        \item The matrix of\/ $\Delta$ with respect to a \textsl{Jordan basis}
        $$
            \big((x-\lambda_1)^{e_{11}},\dots,
            (x-\lambda_1)^{e_{1k_1}},\dots,
            (x-\lambda_r)^{e_{r1}},\dots,
            (x-\lambda_r)^{e_{rk_r}}\big),
        $$
        is
        $$
            \op{diag}(\mathcal J(\lambda_1,e_{11}), 
                \dots,\mathcal J(\lambda_1,e_{1k_1}),
                \dots,\mathcal J(\lambda_r,e_{r,1}), \dots,\mathcal J(\lambda_r,e_{rk_r})),
        $$
        where the polynomials\/ $(x-\lambda_i)^{e_{ij}}$ are the elementary divisors of\/ $\Delta$. This block diagonal matrix is called the \textsl{Jordan canonical form} of~$\Delta$.

        \item If\/ $\kappa$ is algebraically closed, then up to order of the block diagonal matrices, the set of matrices in Jordan canonical form constitutes a set of canonical forms for similarity.
    \end{enumerate}
\end{thm}

\begin{ntn}
    Let\/ $\kappa$ be a field. If\/ $m\in\N$, the splitting field\/\footnote{See \citep{LC}~\S\,Normal Extensions} of\/ $x^m-1$ will be denoted by\/ $\kappa_m$.
\end{ntn}

\begin{cor}\label{cor:triangularizable-cyclotomic}
    Let $M\in M_n(\kappa)$ be such that $M^m=\op{Id}$. Then $M$ is triangularizable in $M_n(\kappa_m)$.
\end{cor}

\begin{proof}
    By hypothesis the minimal polynomial $m_M(x)$ divides $x^m-1$. Therefore, the minimal $m_M$ splits over $\kappa_m$ and so, by the theorem, $M$ is similar to a matrix $J$ in Jordan canonical form. The conclusion follows because $J$ is triangular.
\end{proof}

\begin{thm}\label{thm:Cayley} {\rm [Cayley]}
    Let\/ $A$ and\/ $B$ be two\/ $n\times n$ matrices with coefficients in a field~$\kappa$ such that\/ $AB=BA$. If\/ $f(x,y)=\det(xA-yB)$, then\/ $f(B,A)=0$.
\end{thm}

\begin{proof} {[cf.~Theorem~\ref{thm:Cayley-2}]}
    First observe that after replacing $\kappa$ with an algebraic closure, we may assume that $\kappa$ is algebraically closed.
    
    \textbf{Step 1.} \textit{$A=I$ the identity, $B=\mathcal J(\lambda,e)$ a Jordan block.}
    
    In this case
    $$
        xI-yB = \begin{bmatrix}
                x-\lambda y & 0 & \cdots & \cdots & 0 \\
                -y & x-\lambda y & \ddots &  & \vdots \\
                0 & -y & \ddots & \ddots & \vdots \\
                \vdots & \ddots & \ddots & \ddots & 0 \\
                0 & \cdots & 0 & -y & x-\lambda y
            \end{bmatrix}.
    $$
    Therefore,
    $$
        f(x,y)=\det(xI-yB)=(x-\lambda y)^e
    $$
    and so
    $$
        f(B,A) = f(B,I) = (B-\lambda I)^e = 0
    $$
    because $B-\lambda I$ is strictly triangular, with $1$s in the subdiagonal and zero everywhere else.
    
    \textbf{Step 2.} \textit{$A=I$, $B$ is in Jordan form.} 
    
    Since $B$ is a diagonal of Jordan blocks, the matrix $xI-yB$ is a diagonal of~$r$ blocks $xI_{e_i}-y\mathcal J(\lambda_i,e_i)$. Then $f(x,y)$ is the product of the determinants of said blocks and $f(B,A)$ is the product their determinants evaluated at $x=B$ and $y=I$. As shown in Step~1, we have
    $$
        f(x,y)=\prod_{i=1}^r(x-\lambda_i y)^{e_i}.
    $$
    Thus,
    $$
        f(B,I) = \prod_{i=1}^r(B-\lambda_iI)^{e_i},
    $$
    which is $0$ because it is the product of $r$ block diagonal matrices of the same \textit{shape} as $B$ (i.e., their $i$th block is an $e_i\times e_i$ matrix), where the $i$th block is zero in the $i$th factor $(B-\lambda_iI)^{e_i}$.

    \textbf{Step 3.} \textit{$A=I$, $B$ arbitrary.}

    Let $P$ be such that $J=PBP^{-1}$ is in Jordan form. Step~2 implies that
    $$
        f(x,y) = \det(xI-yB)
            = \det(xI-yJ)=
            \prod_{i=1}^r(x-\lambda_i y)^{e_i}.
    $$
    Thus,
    $$
        f(B,I) = \prod_{i=1}^r(B-\lambda_iI)^{e_i}.
    $$
    Multiplying by $P$ and $P^{-1}$,
    $$
        Pf(B,I)P^{-1}=\prod_{i=1}^r(J-\lambda_iI)^{e_i}=0
    $$
    as explained in Step~2. Since $P$ is invertible, we get $f(B,I)=0$, as desired.

    \textbf{Step 4.} \textit{$A$ invertible, $B$ arbitrary with $AB=BA$.}

    In this case we have
    \begin{equation}\label{eq:step-4}
        f(x,y) = \det(A)\det(xI-yA^{-1}B)
            = \det(A)\prod_{i=1}^r(x-\lambda_i y)^{e_i},
    \end{equation}
    where $\lambda_1,\dots,\lambda_r$ are the eigenvalues of $A^{-1}B$. According to Step~3, we have
    \begin{equation}\label{eq:step-4-2}
        \prod_{i=1}^r(A^{-1}B-\lambda_iI)^{e_i}=0.
    \end{equation}
    Multiplying by $A^{e_1}\cdots A^{e_r}$, we get
    \begin{equation*}
        \prod_{i=1}^rA^{e_i}(A^{-1}B-\lambda_iI)^{e_i}=0
    \end{equation*}
    and given that $A$ commutes with $B$, the four matrices $A$, $A^{-1}$, $B$ and $I$ commute with each other, which allows us to write
    \begin{equation}\label{eq:step-4-3}
        A^{e_i}\overbrace{(A^{-1}B-\lambda_iI)
            \cdots(A^{-1}B-\lambda_iI)}^{e_i\ \rm times} = (B-\lambda_iA)^{e_i}.
    \end{equation}
    Taking into account that $e_1+\cdots+e_r=n$, from \eqref{eq:step-4} and \eqref{eq:step-4-2} we get
    $$
        f(B,A) = \det(A)\prod_{i=1}^r(B-\lambda_iA)^{e_i}
        = \det(A)A^n\prod_{i=1}^r(A^{-1}B-\lambda_iI)^{e_i}=0.
    $$

    \textbf{Step 5.} \textit{General case.}

    Put $A_\zeta=A-\zeta I$. Since $B$ commutes with $A$ it commutes with $A_\zeta$. Define
    $$
        f_\zeta(x,y) = \det(xA_\zeta-yB).
    $$
    Then $f_\zeta(x,y)$ is a polynomial in $x$, $y$ and $\zeta$ of degree at most $n$ in each of these variables. According to Step~4, $f_\zeta(x,y)$ satisfies
    \begin{equation}\label{eq:step-5}
        f_\zeta(B,A_\zeta) = 0,
    \end{equation}
    whenever $\zeta$ is not an eigenvalue of $A$. Since $\kappa$ is infinite (it is algebraically closed), condition \eqref{eq:step-5} implies that $f_\zeta(B,A_\zeta)$ is a matrix whose entries, which are polynomials in $\zeta$ that are zero or have degree at most $n$, must be zero in~$\kappa[\zeta]$. In particular, the evaluation at $\zeta=0$ is the zero matrix. Therefore,
    $$
        f(B,A) = f_\zeta(B,A_\zeta)|_{\zeta=0} = 0.
    $$
\end{proof}

\begin{lem}\label{lem:common-eigenvector}
    Let\/ $\kappa$ be algebraically closed. If\/ $A_1,\dots, A_m\in M_n(\kappa)$ are mutually commutative, then they have a common eigenvector.
\end{lem}

\begin{proof}
    We proceed by induction on $m$. The base case $m=1$ is trivial. For $m > 1$, let $\V$ denote the subspace of common eigenvectors of $A_1, \dots, A_{m-1}$. We claim that $\V$ is invariant under $A_m$. 

    To verify this, take any $v \in \V$ and $1 \leq i < m$. By definition, $A_i v = \lambda_i v$ for some $\lambda_i \in \kappa$. Consequently, we have:
    $$
        A_i A_m v = A_m A_i v = \lambda_i A_m v,
    $$
    which shows that $A_m v$ is an eigenvector of $A_i$. Hence, $A_m v \in \V$, proving that $\V$ is $A_m$-invariant.
    
    Now, let $B$ denote the restriction of the endomorphism $x \mapsto A_m x$ to $\V$. Since $\kappa$ is algebraically closed, we can choose an eigenvalue $\lambda$ of $B$ and an associated eigenvector $w \in \V$. By construction, $w$ is an eigenvector of $A_m$ as well as of $A_1, \dots, A_{m-1}$. This completes the proof.
\end{proof}


\begin{prop}\label{prop:simultaneous-traingularization}
    Let\/ $\kappa$ be algebraically closed. If $A_1,\dots, A_m \in M_n(\kappa)$ are mutually commutative, then the matrices are simultaneously triangularizable.
\end{prop}

\begin{proof}
    Let $v$ be a common eigenvector of $A_1,\dots,A_m$. Let $\basis B=(v_1,\dots,v_{n-1},v)$ be a basis of $\kappa^n$. In this basis each $A_i$ adopts the form
    $$
        [A_i]_{\basis B} = \begin{bmatrix}
                A_i'    &0\\
                *   &\lambda_i
            \end{bmatrix}
    $$
    Since
    $$
        [A_i]_{\basis B}[A_j]_{\basis B} = \begin{bmatrix}
                A_i'A_j'    &0\\
                *   &\lambda_i\lambda_j
            \end{bmatrix}
    $$
    we see that $A_i'$ and $A_j'$ commute and the conclusion follows by induction on~$n$.
\end{proof}


\begin{thm}\label{thm:eigenvalues-of-f(A,B)}
    Suppose that\/ $\kappa$ is algebraically closed. Let\/ $A, B \in \kappa^{n \times n}$ be commuting matrices and\/ $f \in \kappa[x, y]$ a polynomial in two variables. Denote by\/ $(\lambda_1, \dots, \lambda_n)$ and\/ $(\mu_1, \dots, \mu_n)$ the eigenvalues of\/ $A$ and\/ $B$ counted with multiplicity. Then, after reindexing the\/ $\lambda_i$ and the\/ $\mu_j$ the eigenvalues of\/ $f(A,B)$ counted with multiplicity are:
    $$
        (f(\lambda_1,\mu_1),\dots,f(\lambda_n, \mu_n)).
    $$
\end{thm}

\begin{proof}
    According to the previous proposition, we may assume that $A$ and $B$ are lower triangular. Moreover, the diagonals of these matrices are respectively $(\lambda_1,\dots,\lambda_n)$ and\/ $(\mu_1,\dots,\mu_n)$. Then, given any exponents $i,j\ge0$ the matrices $A^i$ and $B^j$ are lower triangular with diagonals $(\lambda_1^i, \dots,\lambda_n^i)$ and\/ $(\mu_1^j,\dots,\mu_n^j)$. Thus $f(A,B)$ is lower triangular with diagonal $(f(\lambda_1,\mu_1),\dots,f(\lambda_n,\mu_n))$.    
\end{proof}

\begin{lem}\label{lem:null-diagonal-product}
    Let $A=A_1\cdots A_n$ be the product of $n\times n$ lower triangular matrices mutually commutative. Then $A=0$ if, and only if, $\diag(A)=(0,\dots,0)$.
\end{lem}

\begin{proof}
    Since we only need to show the \textit{if\/} part, we may assume that the $i$th element of the diagonal of $A_i$ is zero.
    
    Under this assumption, we claim that, given $1 \leq k \leq n$, the columns $k,\dots, n$ of the product $A_k \cdots A_n$ are zero.
    
    We proceed by reverse induction on $k$. The claim is true for $k = n$ because the last column of $A_n$ is zero.
    
    Let $e_1, \dots, e_n$ denote the elements of the canonical basis. Suppose the claim holds for $k$. Take $k \leq j \leq n$. Then
    $$
    A_k \cdots A_n e_j = 0,
    $$
    and consequently,
    $$
    A_{k-1} A_k \cdots A_n e_j = 0.
    $$
    It remains to verify the above for $j = k-1$. Since $A_{k-1} e_{k-1} \in \langle e_k, \dots, e_n \rangle$, we can write
    $$
    A_{k-1} e_{k-1} = c_k e_k + \cdots + c_n e_n.
    $$
    Therefore,
    $$
    A_{k-1} A_k \cdots A_n e_{k-1} =
        A_k \cdots A_n A_{k-1} e_{k-1}
        = \sum_{j=k}^n c_j A_k \cdots A_n e_j
        = 0.
    $$
    
    The claim follows, and for $k = 1$, so does the lemma.
\end{proof}

\begin{thm}\label{thm:nilpotent-n-product}
    Let\/ $A_1,\dots,A_n$ be\/ $n$ mutually commutative\/ $n\times n$ matrices. Then their product\/ $A_1\cdots A_n$ is zero if, and only if, it is nilpotent. 
\end{thm}

\begin{proof}
    It is enough to show that the product is zero, provided it is nilpotent. In consequence, we may assume that $\kappa$ is algebraically closed. Thus, by Proposition~\ref{prop:simultaneous-traingularization}, we may further suppose that $A_1,\dots,A_n$ are triangular. And since $A=A_1\cdots A_n$ is nilpotent and triangular, $\diag(A)=(0,\dots,0)$. Thus, $A=0$ by Lemma~\ref{lem:null-diagonal-product}.
\end{proof}

\begin{rem}
    Proposition~\ref{prop:simultaneous-traingularization} has a very simple corollary: \textit{Every square matrix is triangularizable over an algebraic closure of the base field.}. This fact allows us to prove the Cayley-Hamilton theorem as follows.

    If $A$ is triangular with diagonal $(\lambda_1,\dots,\lambda_n)$ then
    $$
        \chi_A(x) = \det(xI-A)
            = (x-\lambda_1)\cdots(x-\lambda_n).
    $$
    Therefore,
    $$
        \chi_A(A) = (A-\lambda_1I)\cdots(A-\lambda_nI),
    $$
    which is zero by the lemma.
    
    In the general case, after replacing $\kappa$ with an algebraic closure, we can take~$P$ invertible such that $A'=PAP^{-1}$ is triangular. Then $\chi_A(x)=\chi_{A'}(x)$ and so
    $$
        \chi_A(A)=P\chi_A(A')P^{-1}
            =P\chi_{A'}(A')P^{-1} = 0.
    $$
\end{rem}

\bigskip

We can now provide a second proof of Cayley's Theorem~\ref{thm:Cayley}.

\begin{thm}\label{thm:Cayley-2} {\rm [Cayley]}
    Let\/ $A$ and\/ $B$ be two\/ $n\times n$ matrices with coefficients in a field~$\kappa$ such that\/ $AB=BA$. If\/ $f(x,y)=\det(xA-yB)$, then\/ $f(B,A)=0$.
\end{thm}

\begin{proof}
    After replacing $\kappa$ with an algebraic closure, we may assume that $A$ and $B$ are lower triangular. Put $\diag(A)=(\lambda_1,\dots,\lambda_n)$ and $\diag(B)=(\mu_1,\dots,\mu_n)$. Since $A$ and $B$ are triangular, $xA-yB$ is triangular with diagonal
    $$
        \op{diag}(xA-yB)=(\lambda_1x-\mu_1y,\dots,
            \lambda_nx-\lambda_ny).
    $$
    Hence,
    \begin{equation}\label{eq:f(x,y)}
        f(x,y)=(\lambda_1x-\mu_1y)\cdots
            (\lambda_nx-\lambda_ny)    
    \end{equation}
    and so
    $$
        f(B,A) = (\lambda_1B-\mu_1A)
            \cdots(\lambda_nB-\mu_nA).
    $$
    By Theorem~\ref{thm:eigenvalues-of-f(A,B)}, the eigenvalues of $f(B,A)$ are $(f(\mu_1,\lambda_1),\dots,f(\mu_n,\lambda_n))$, which are all zero by \eqref{eq:f(x,y)}. Thus, $f(B,A)$ is nilpotent and the result is a direct consequence of Theorem~\ref{thm:nilpotent-n-product}.
    
\end{proof}

\section{Exercises}

\begin{exr}
    We have seen that any $\Delta\in\End_\kappa(\V)$ can be used to make into an $\kappa[x]$-module. Does every module over $\kappa[x]$ come from some $\Delta$. Explain
\end{exr}

\begin{solution}
    Yes, it does. Given an action of $\kappa[x]$ on $\V$ define
    \begin{align*}
        \Delta\colon\V&\to\V\\
        v&\mapsto x\cdot v.
    \end{align*}
    Then $\Delta$ is $\kappa$-linear because
    \begin{align*}
        \Delta(0) &= x\cdot 0=0\quad\text{and}\\
        \Delta(v+\lambda w) &= x\cdot(v+\lambda w)=x\cdot v+\lambda x\cdot w = \Delta(v)+\lambda\Delta(w).
    \end{align*}
    Moreover, $\Delta$ redefines the very same action that originated it.
\end{solution}

\begin{exr}
    Prove that a matrix is nonderogatory if, and only if, it is similar to a companion matrix.
\end{exr}

\begin{solution}
    Since companion matrices are nonderogatory, the \textit{if\/} part is clear. Conversely, if $A$ is nonderogatory with minimal $\chi_A=p_1^{e_1}\cdots p_r^{e_r}$, Theorem~\ref{thm:Delta-cyclic-equivalences} implies that $\V_A$ is a direct sum of cyclic submodules of order $p_i^{e_i}$. Therefore, by Theorem~\ref{thm:companion-of-product}, we have
    $$
        A\sim\op{\diag}(C[p_1^{e_1}],\dots,C[p_r^{e_r}])
            \sim C[p_1^{e_1}\cdots p_r^{e_r}] = C[\chi_A].
    $$
\end{solution}

\begin{exr}${}$
    \begin{enumerate}[\rm a)]
        \item Show that if\/ $A$ and $B$ are matrices, at least one of which is invertible, then\/ $AB$ and\/ $BA$ are similar.

        \item What do the matrices
        $$
            A = \begin{bmatrix}
                    1 & 0 \\
                    0 & 0
                \end{bmatrix} \quad \text{and} \quad
            B = \begin{bmatrix}
                    0 & 1 \\
                    0 & 0
                \end{bmatrix}
        $$
        have to do with this issue?

        \item Show that even without the assumption on invertibility the matrices\/ $AB$ and\/ $BA$ have the same characteristic polynomial.
        
        \textrm{\rm Hint: Write 
        $$
            A = PI_{nr}Q
        $$
        where $P$ and $Q$ are invertible and $I_{nr}$ is an $n \times n$ matrix that has the $r \times r$ identity in the upper left-hand corner and $0$'s elsewhere. Write $B' = QBP$. Compute $AB$ and $BA$ and find their characteristic polynomials.}
    \end{enumerate}
\end{exr}

\begin{solution}${}$
    \begin{enumerate}[\rm a)]
        \item If $A$ is invertible, then $BA = A^{-1}(AB)A$. Hence, $BA\sim AB$.

        \item In this case $AB=B$ and $BA=0$, which are not similar because $0$ is only similar to itself. Note however that
        $$
            \chi_{AB}(x)=\chi_B(x)=x^2=\chi_0(x),
        $$
        i.e., $AB$ and $BA$ have the same characteristic polynomial.

        \item The existence of $P$ and $Q$ comes from applying elementary row operations on $A$ and then elementary column operations on $PA$ until we get $I_{nr}$, for $r=\rank(A)$. Then
        \begin{align*}
            AB &= PI_{nr}QQ^{-1}B'P^{-1}=PI_{nr}B'P^{-1}\\
            BA &= Q^{-1}B'P^{-1}PI_{nr}Q=Q^{-1}B'I_{nr}Q.
        \end{align*}
        Let $C$ be the $r\times r$ upper left corner of $B'$. Then
        $$
            I_{nr}B' = \begin{bmatrix}
                C&*\\
                0&0
            \end{bmatrix}
            \quad\text{and}\quad
            B'I_{nr} = \begin{bmatrix}
                C&0\\
                *&0
            \end{bmatrix}.
        $$
        Therefore,
        $$
            \chi_{I_{nr}B'}(x) = \chi_C(x)x^{n-r}
                = \chi_{B'I_{nr}}(x)
        $$
        and
        $$
            \chi_{AB}=\chi_{I_{nr}B'}=\chi_{B'I_{nr}}=\chi_{BA}.
        $$
    \end{enumerate}
\end{solution}

\begin{exr}
    Suppose that the minimal polynomial of\/ $\Delta\in\End_\kappa(\V)$ is irreducible. What can you say about the dimension of\/~$V$?
\end{exr}

\begin{solution}
    There exists an integer $e$ such that $\chi_\Delta=m_\Delta^e$. Thus, 
    $$
        \dim\V=\deg(m_\Delta^e)=e\deg m_\Delta,
    $$
    i.e., $\deg m_\Delta\mid\dim\V$.
\end{solution}

\begin{exr}
    Let\/ $\Delta\in\End_\kappa(\V)$ where\/ $\V$ is finite-dimensional. Suppose that\/ $p(x)$ is an irreducible factor of the minimal polynomial of\/ $\Delta$. Suppose further that\/ $v,w\in\V$ have the property that\/ $\ord(v) = \ord(w) = p(x)$. Prove that\/ $v = f(\Delta)w$ for some polynomial\/ $f(x)$ if, and only if, $w = g(\Delta)v$ for some polynomial\/~$g(x)$.
\end{exr}

\begin{solution}
    Suppose that $v=f(\Delta)w$. Suppose that $p\mid f$. Then, we would have $f=pq$ for some $q\in\kappa[x]$ and so $v=q(\Delta)p(\Delta)w=0$, in contradiction with the assumption that $\ord(v)=0$. Thus, $p\perp f$ and we can write $1=hp+gf$ for some $h,g\in\kappa[x]$. It follows that 
    $$
        w=h(\Delta)p(\Delta)w+g(\Delta)f(\Delta)w = g(\Delta)v.
    $$
\end{solution}

\begin{exr}
    Let\/ $\varphi$ and\/ $\phi$ two elements of $\End_\kappa(\V)$. Suppose that both\/ $\varphi$ and\/ $\phi$ are diagonalizable. Then\/ $\varphi\phi=\phi\varphi$ if, and only if, there exists a basis\/~$\basis B$ of\/ $\V$ such that\/ $[\varphi]_{\basis B}$ and\/ $[\phi]_{\basis B}$ are diagonal.
\end{exr}

\begin{solution}
    \begin{description}
        \item[\rm\textit{Only if\/})] Take an eigenvector of $\varphi$ with eigenvalue $\lambda$. Then $\varphi\phi v=\phi\varphi v=\lambda\phi v$. In other words, the eigenspaces of $\varphi$ are $\phi$-invariant. 
        
        Let $E_1,\dots,E_r$ denote the eigenspaces of $\varphi$. For each $1\le i\le r$, let $\varphi_i\subseteq\varphi$ and $\phi_i\subseteq\phi$ be the morphisms induced on $E_i$. Then $\varphi_i\phi_i=\phi_i\varphi_i$. If $r>1$ induction on $\dim\V$ implies that there exists a basis $\basis B_i$ of $E_i$ where $[\varphi_i]_{\basis B_i}$ and $[\phi_i]_{\basis B_i}$ are diagonal. Thus, the concatenation $\basis B=\basis B_1,\dots,\basis B_r$ diagonalizes both endomorphism.

        In the case where $r=1$, $\varphi$ is an homotecy and $[\varphi]_{\basis B}$ is scalar in any basis~$\basis B$.

        \item[\rm\textit{if\/} part)] Diagonal matrices commute. Therefore, $[\varphi]_{\basis B}$ and $[\phi]_{\basis B}$ commute and so $[\varphi\phi]_{\basis B}=[\phi\varphi]_{\basis B}$, which implies $\varphi\phi=\phi\varphi$.
    \end{description}
\end{solution}